# 卒業研究
VGG16という畳み込みニューラルネットワークをファインチューニングし、教室後方から撮影された講義中の学生の画像から、7種類の受講状態を識別する研究に取り組んだ。
手動で切り出した画像とYOLOモデルで切り出した画像の両方で実験を行い、Grad-CAMを用いてVGG16が画像のどの部分に注目して識別しているかを可視化した。

大学生を1人ずつ着席させ、7種類の受講状況をポジション.pngの番号順にそれぞれ動画で撮影し、これを6人分撮影した。

(容量の関係で掲載は割愛、6人×9ポジション、データが欠落したものがあり切り出し元から除いている)

受講状態のラベルは
  0:正常（姿勢を正し聞いている）、
  1:机の下でスマホを操作、
  2:脇を開けてのスマホ操作、
  3:脇を閉じてのスマホ操作、
  4:突っ伏し、
  5:俯いて寝ている、
  6:上向きで寝ている
  
動画の各受講状況のスクリーンショットを切り出し元とし、

・学生を手動で切り出した画像データ：手動

・Yoloの姿勢推定モデルで推定した骨格をプロットした画像を保存し用いて、Yoloの物体検出モデルで切り出した画像データ

：YOLO (～().pngの名前の画像は未使用)

を使用(画像のナンバリングはa受講状態ラベル-学生-ポジション、掲載時に手動YOLOとも圧縮)

本研究ではGrad-CAMに必要な識別モデルの畳み込み層の出力を数値として取得できなかったため、
識別モデルの学習時の過学習を起こしたとみられる直前の80エポックでの重みを保存し、同じ条件のモデルを作り、それに重みを読み込ませたものの出力を使用した

そのため、Grad-CAM用モデルでの学習時の正解率と損失はExcelファイルで手動により算出した

モデルの学習、分類、Grad－CAM適用を行ったvgg_(1)_fixed、YOLOによる切り出し、骨格推定・プロットを行ったsiseiのプログラムファイルを添付している
