# 卒業研究
VGG16という畳み込みニューラルネットワークをファインチューニングし、教室後方から撮影された講義中の学生の画像から、7種類の受講状態を識別する研究に取り組みました。
手動で切り出した画像とYOLOモデルで切り出した画像の両方で実験を行い、Grad-CAMを用いてVGG16が画像のどの部分に注目して識別しているかを可視化しました。

大学生を1人ずつ着席させ、7種類の受講状況をポジション.pngの番号順にそれぞれ動画で撮影し、これを6人分撮影した。(「動画」ファイル、6人×9ポジション)

受講状態のラベルは
  0:正常（姿勢を正し聞いている）、
  1:机の下でスマホを操作、
  2:脇を開けてのスマホ操作、
  3:脇を閉じてのスマホ操作、
  4:突っ伏し、
  5:俯いて寝ている、
  6:上向きで寝ている
  
動画の各受講状況のスクリーンショットを切り出し元とし、
・学生を手動で切り出した画像データ：手動
・Yoloの姿勢推定モデルで推定した骨格をプロットした画像を用いて、Yoloの物体検出モデルで切り出した画像データ：YOLO (～().pngの名前の画像は未使用)
を使用(画像のナンバリングはa受講状態ラベル-学生-ポジション)

本研究ではGrad-CAMに必要な識別モデルの畳み込み層の出力を数値として取得できなかったため，
識別モデルの学習時の過学習を起こしたとみられる直前の80エポックでの重みを保存し，同じ条件のモデルを作り，それに重みを読み込ませたものの出力を使用している
そのため、Grad-CAM用モデルの正解率と損失はExcelファイルで手動により算出しています
